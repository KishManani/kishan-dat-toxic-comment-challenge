{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGETS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_preds_to_df(preds):\n",
    "    target_probs = pd.DataFrame([[c[1] for c in preds[row]] for row in range(len(preds))]).T\n",
    "    target_probs.columns = TARGETS\n",
    "    return target_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_roc_auc(y_true, y_score, verbose=False):\n",
    "    ''' Compute roc auc for each target and then average them\n",
    "    y_true - dataframe of true targets\n",
    "    y_score - dataframe of predicted target\n",
    "    '''\n",
    "    roc_scores = dict()\n",
    "    for target in TARGETS:\n",
    "        roc_score = roc_auc_score(y_true=y_true[target], y_score=y_score[target])\n",
    "        roc_scores[target] = roc_score\n",
    "\n",
    "    mean_roc_score = np.mean(list(roc_scores.values()))\n",
    "    \n",
    "    if verbose: \n",
    "        print('Mean ROC AUC overall all targets: {}'.format(mean_roc_score))\n",
    "    \n",
    "    return mean_roc_score, roc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_model(model, train_text, train_targets, test_text, test_targets):\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(train_text, train_targets)\n",
    "    \n",
    "    # Predict\n",
    "    preds_train = model.predict_proba(train_text)\n",
    "    preds_test = model.predict_proba(test_text)\n",
    "    \n",
    "    # Transform predictions to dataframes\n",
    "    preds_train_df = transform_preds_to_df(preds_train)\n",
    "    preds_test_df = transform_preds_to_df(preds_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    mean_roc_score_train, roc_scores_train = multi_roc_auc(y_score=preds_train_df.loc[:,TARGETS], y_true=train_targets)\n",
    "    mean_roc_score_test, roc_scores_test = multi_roc_auc(y_score=preds_test_df.loc[:,TARGETS], y_true=test_targets)\n",
    "    print('Train score: {} Test score: {}'.format(mean_roc_score_train, mean_roc_score_test))\n",
    "    print('Individual train score: {} Individual test score: {}'.format(roc_scores_train, roc_scores_test))\n",
    "    \n",
    "    return preds_train_df, preds_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/external/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir + 'train.csv')\n",
    "test_df = pd.read_csv(data_dir + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform text to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_features=800000)\n",
    "train_text = tfidf.fit_transform(train_df['comment_text'])\n",
    "val_text = tfidf.transform(val_df['comment_text'])\n",
    "test_text = tfidf.transform(test_df['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and score on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "           n_jobs=-1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiOutputClassifier(LogisticRegression(), n_jobs=-1)\n",
    "model.fit(train_text, train_df[TARGETS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_train = model.predict_proba(train_text)\n",
    "preds_val = model.predict_proba(val_text)\n",
    "preds_test = model.predict_proba(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train_df = transform_preds_to_df(preds_train)\n",
    "preds_val_df = transform_preds_to_df(preds_val)\n",
    "preds_test_df = transform_preds_to_df(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_roc_score, roc_scores = multi_roc_auc(y_score=preds_train_df.loc[:,TARGETS], y_true=train_df.loc[:,TARGETS])\n",
    "mean_roc_score, roc_scores = multi_roc_auc(y_score=preds_val_df.loc[:,TARGETS], y_true=val_df.loc[:,TARGETS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multiple models and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing: MultiOutputClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "           n_jobs=-1)\n",
      "Train score: 0.9934007237257818 Test score: 0.9756044351143994\n",
      "Individual train score: {'toxic': 0.9934007237257818, 'severe_toxic': 0.9934007237257818, 'obscene': 0.9934007237257818, 'threat': 0.9934007237257818, 'insult': 0.9934007237257818, 'identity_hate': 0.9934007237257818} Individual test score: {'toxic': 0.9756044351143994, 'severe_toxic': 0.9756044351143994, 'obscene': 0.9756044351143994, 'threat': 0.9756044351143994, 'insult': 0.9756044351143994, 'identity_hate': 0.9756044351143994}\n",
      "Training and testing: MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "           n_jobs=-1)\n",
      "Train score: 0.9999184719040685 Test score: 0.8718090553644849\n",
      "Individual train score: {'toxic': 0.9999184719040685, 'severe_toxic': 0.9999184719040685, 'obscene': 0.9999184719040685, 'threat': 0.9999184719040685, 'insult': 0.9999184719040685, 'identity_hate': 0.9999184719040685} Individual test score: {'toxic': 0.8718090553644848, 'severe_toxic': 0.8718090553644848, 'obscene': 0.8718090553644848, 'threat': 0.8718090553644848, 'insult': 0.8718090553644848, 'identity_hate': 0.8718090553644848}\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "          'LogisticRegression': MultiOutputClassifier(LogisticRegression(), n_jobs=-1),\n",
    "          'RandomForest':  MultiOutputClassifier(RandomForestClassifier(n_estimators=10), n_jobs=-1)\n",
    "          }\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print('\\n Training and testing: {}'.format(model))\n",
    "    preds_train_df, preds_test_df = train_and_test_model(model=model, train_text=train_text, \n",
    "                                                         train_targets=train_df.loc[:,TARGETS],\n",
    "                                                         test_text=val_text, \n",
    "                                                         test_targets=val_df.loc[:,TARGETS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.concat([test_df['id'], preds_test_df], axis=1)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.to_csv('../data/processed/submission_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_baseline.pickle']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'logistic_baseline.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
